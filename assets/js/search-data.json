{
  
    
        "post0": {
            "title": "Cross validation pitfalls",
            "content": "There is often a discussion about what is the science part of data science. I like Karl Popper&#39;s definition of the scientific process as a process of falsifying the hypothesis and evaluating predictions. Karl Popper was born in 1902 and he was very close to the major progress in different fields - he observed Einstein coming up with the Relativity Theory and his desire to repeatedly experimentally test his theory even after it was already widely accepted. Popper also observed Sigmund Freud who approach the theories from a different side - he observed some phenomenon and come up with a theory to explain it, but no hypothesis testing, no prediction evaluation was part of the process, Karl Popper called it pseudo-science. . To me, there is a strong parallel between this story and the methodology used commonly in data science called cross-validation. We cannot just create a model based on past observations without any evaluation of unseen data. Theory/model which can perfectly explain a past observation is of no value if it cannot be used for prediction since there is an infinite set of concurrent perfect fits - perfect explanations. Freud&#39;s theories are today considered good maybe for leisure reading but not as a scientific theory as opposed to Einstein&#39;s work so we should rather test our models as thoroughly and honestly as possible to strive for a successful long-running model in production. It could look incredibly dumb to say that by calling cross_validate(X, y, 5) we are doing science, but it doesn&#39;t really matter how simple is to evaluate prediction if you are doing it right. Despite awesome tools like the sklearn there are still many traps we can fall into and this small blog-post can show how to prevent at least some of them and give few tricks I found useful in practice. . We can start with the simplest approch - keeping one holdout set with train_test_split. . from sklearn.model_selection import train_test_split . Let&#39;s load some data for regression. . X, y= make_regression_with_trend(n_samples=1000, n_features=4, n_informative=3, bias=2, noise=5, random_state=0) print(X.shape, &#39;the full dataset shape&#39;) . (1000, 5) the full dataset shape . Keep 30 % of dataset for evaluation. . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) print(X_train.shape, &#39;the train dataset shape&#39;) print(X_test.shape, &#39;the est dataset shape&#39;) . (700, 5) the train dataset shape (300, 5) the est dataset shape . And fit the linear regression. . model = LinearRegression() model.fit(X_train, y_train).score(X_test, y_test) . 0.7438210243365382 . R2 = 0.73 not bad right? Let&#39;s just try it one more time. . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, shuffle=False) . model.fit(X_train, y_train).score(X_test, y_test) . 0.6277279246349733 . What happened? As usual, the devil is in the detail, and details in the case of functions are default parameters. In the case of train_test_split the parameter is shuffle default is True so if your data has any inherent structure (like sensor values from the different time), you run into the risk that you will have an artificially good prediction because you leaked information about temporal structure into your training set. . You could assume that shuffle is True for all other splitting functions but you would be wrong. Let say that you want to have more robust cross-validation so you want more splits so for a regression problem natural selection would be Kfold, but it has shuffle with default False. . Temporal structure in dataset . The problem of having data with some structure (often temporal) which would result in data leakage when shuffling the data is in my experience far more common than is usually acknowledged. Most of the training data are collected over time and, likely, the relationships are not fixed. Take customer/sales related data - trends, behaviors, social paradigm everything is constantly changing so trying to evaluate performance with shuffling the data will very likely result in overfitting (think i.e. about the impact of covid on sales of toilet paper). At first look, it could seem that the temporal changes are not that much connected with machine-generated data (sensors...), and shuffling is just a fine strategy. It could be, but very often it&#39;s not - sensors, devices, machines are changed and operated by humans with an unstable performance which very often leads to trends visible in the data. The simplest thing to do is to just plot your dependent variable over time and check whether there are any visible trends and try to cross-validate with shuffling and without to see if there are any significant differences. If you spot that the data are not really perpendicular to the time dimension we should cross-validate accordingly. Sklearn offers TimeSeriesSplit. Be sure to use version =&gt; 0.24, before this class had just argument n_splits and max_train_size which made it quite useless. From version 0.24 you can use test_size to specify your horizon, max_train_size to clip your data (maybe you want to keep just 2 last years of sales data for training because older are not relevant anymore), gap argument is very useful to evaluate multiple splits more independent. The following code nicely illustrates how individual parameters works: . cv = TimeSeriesSplit(n_splits = 3, max_train_size=10, test_size=3, gap=2) for train_index, test_index in cv.split(X): print(&quot;TRAIN:&quot;, train_index, &quot;TEST:&quot;, test_index) . TRAIN: [979 980 981 982 983 984 985 986 987 988] TEST: [991 992 993] TRAIN: [982 983 984 985 986 987 988 989 990 991] TEST: [994 995 996] TRAIN: [985 986 987 988 989 990 991 992 993 994] TEST: [997 998 999] . Vizualize the predictions from cross validation . One thing I don&#39;t particularly like about sklearn is that in most cases the prediction from GridSearch and cross-validation are not available and we are left just with few statistics. One workaround for that could be a custom implementation of a scoring function that tracks the data for you as it&#39;s done i.e. in hcrystalball package (disclosure I&#39;m the maintainer of this project https://github.com/heidelbergcement/hcrystalball/blob/master/src/hcrystalball/metrics/_scorer.py), but it currently works just with hcrystalball wrapper and not with parallel computing. One solution is to use cross_val_predict which is returning predictions for all folds, but it doesn&#39;t work with TimeSeriesSplit thus very often the simplest solution is to use KFold with the sorted dataset by datetime without shuffling which we can get just by specifying cv parameter of cross_val_predict as an integer. . predictions_unshuffled = cross_val_predict(model, X_train, y_train, cv=3) pd.DataFrame({&#39;predictions_unshuffled&#39;: perd_unshuffled,&#39;actuals&#39;:y_train}).plot.scatter(&#39;predictions_unshuffled&#39;, &#39;actuals&#39;) . *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. . &lt;AxesSubplot:xlabel=&#39;predictions_unshuffled&#39;, ylabel=&#39;actuals&#39;&gt; .",
            "url": "https://pavelkrizek.github.io/ml-blog/2021/11/01/cross_validation.html",
            "relUrl": "/2021/11/01/cross_validation.html",
            "date": " • Nov 1, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Statistics 101",
            "content": "Personally, I think that the hardest thing in starting the journey in data science is actually to answer the question: where to start? The majority of people are concerned with mathematics - how much I actually need it? What kind of math? How deep I should go? Almost all resources agree that you should know some stats, linear algebra, calculus, optimization and usually they throw at you a few books because reading books doesn&#39;t hurt right? I think that they could actually hurt because if you dedicate too much mental energy and time understanding the books you don&#39;t have time to focus on things that actually drive your value as a data scientist - coding, data wrangling skills, familiarity with basic tech (git, SQL, docker...). 5 hours practicing pandas could have an incredible impact on the value you can bring as a data scientist, 5 hours spending on determinants have on the other hand doubtful value at this stage. But is the recommendation really never look at math because you don&#39;t need it? . Not at all, but we should take into account something that economist call the law of diminishing returns (https://en.wikipedia.org/wiki/Diminishing_returns). Simply put, any additional hour you put into learning some particular skill will be worth less and less - just because you already picked all low hanging fruit. Take for example the pandas data wrangling skills, if you spend 1 additional hour with it after a couple of years you are using it, you will not change the way how you use it, and probably not discover anything new because you already mastered most of it. How is it related to learning math for data science? In the beginning, it has just quite a bad ratio in cost-benefit analyses, but over time you learn pandas, sklearn, sql... and the math will again be in a better position so you can finally pick up the book about Linear Algebra from Gilbert Strang so heard so much about. This would unfortunately mean that you maybe don&#39;t touch any math in the first few months at all. This philosophy of hands-on first experience starts to be popular with projects such as fastai which helps you to build state-of-the-art DL models with a reasonable level of understanding without any math. . Although I like the approach, in general, I think that it would be nice to have at least a little bit of math which could have a better cost/benefit ratio at the beginning so it would be worth learning it already as a beginner. This is my attempt to provide the &quot;math&quot; basics with a good cost/benefit ratio which was useful for my data science practice and I would like to have it when I started. . Everything that follows is the basic stats together with the basics of linear models. If you understand it deep enough the relationships between these two, your intuition about linear models and data analyses will grow rapidly and help you to discover flaws in your analyses/modeling more quickly. It can also help to guide modeling and interpretation. . Simple statistics . I would recommend simply memorize the following few basic statiscs . . Sample mean . $$ bar{x}= frac{1}{n} sum_{i=1}^{n} x_{i} $$ Sample varience . $$ s_{y}^{2}= frac{1}{n-1} sum_{i=1}^{n} left(y_{i}- overline{y} right)^{2} $$ Sample standard deviation . $$ s_{y}={ sqrt{ frac{1}{n-1} sum_{i=1}^{n} left(y_{i}- overline{y} right)^{2}}} $$ Sample covarience . $$ s_{x y}= frac{1}{n-1} sum_{i=1}^{n} left(x_{i}- overline{x} right) left(y_{i}- overline{y} right) $$ Pearson correlation coeficient . $$ r_{x y}= frac{s_{x y}}{s_{x} s_{y}} $$ Pearson correlation coeficient in long form . $$ r_{x y} = frac{ sum_{i=1}^{n} left(x_{i}- overline{x} right) left(y_{i}- overline{y} right)}{ sqrt{ sum_{i=1}^{n} left(x_{i}- overline{x} right)^{2} sum_{i=1}^{n} left(y_{i}- overline{y} right)^{2}}} $$ Z - score (standardization of variable to zero mean and unit varience) . $$ z_{i}= frac{x_{i}- overline{x}}{s_{x}} $$ Pearson correlation coeficient could be also written as the average of the sum of the cross-products of z-scores . $$ r_{x y}= frac{1}{n-1} sum_{i=1}^{n} z_{x, i} cdot z_{y, i} $$ Nice way how to explore correlation and enhance your intuition https://rpsychologist.com/d3/correlation/ . . Normal equations for multiple linear regression . Multiple regression . $$ y_{i} = beta_{1}+ beta_{2} x_{2 i}+ beta_{3} x_{3 i}+ cdots+ beta_{k} x_{k i}+ varepsilon_{i} quad(i=1, cdots, n) $$ Multiple regression - matrix notation . $$ y=X beta+ varepsilon $$ If b is estimate of $ beta$ . $$ y=X b+ varepsilon $$ Residuals . $$ e=y-X b $$ Sum of squares of the residuals . $$ begin{aligned} S(b) &amp;= sum e_{i}^{2}=e^{ prime} e=(y-X b)^{ prime}(y-X b) &amp;=y^{ prime} y-y^{ prime} X b-b^{ prime} X^{ prime} y+b^{ prime} X^{ prime} X b end{aligned} $$ The least squares estimator is obtained by minimizing S(b) . $$ frac{ partial S}{ partial b}=-2 X^{ prime} y+2 X^{ prime} X b $$ We set these derivatives equal to zero, which gives the normal equation . $$ X^{ prime} X b=X^{ prime} y $$ Solving this for b we get normal equation . $$ b= left(X^{ prime} X right)^{-1} X^{ prime} y $$ . Relationship between regression and correlation coeficient for univariate case . b for simple regression (univariate x) could be written as: . $$ b= frac{ sum left(x_{i}- overline{x} right) left(y_{i}- overline{y} right)}{ sum left(x_{i}- overline{x} right)^{2}} $$ $$ b= frac{s_{x y}}{s_{x}^{2}} $$ $$ b=r_{x y} frac{s_{y}}{s_{x}} $$ $$ frac{ sum left(x_{i}- bar{x} right) left(y_{i}- bar{y} right)}{ sum left(x_{i}- bar{x} right)^{2}}= frac{ sum_{i=1}^{n} left(x_{i}- bar{x} right) left(y_{i}- bar{y} right)}{ sqrt{ sum_{i=1}^{n} left(x_{i}- bar{x} right)^{2} sum_{i=1}^{n} left(y_{i}- bar{y} right)^{2}}} * frac{ sqrt{ frac{1}{n-1} sum_{i=1}^{n} left(y_{i}- bar{y} right)^{2}}}{ sqrt{ frac{1}{n-1} sum_{i=1}^{n} left(x_{i}- bar{x} right)^{2}}} $$ If they have the same variance (are scaled): . $$ sum left(x_{i}- overline{x} right)^{2} = sum left(y_{i}- overline{y} right)^{2} $$ Then . $$ b=r_{x y} $$ Check empirically that b == coeficient of correalation if x std. == y std . df = tm.makeTimeDataFrame(freq=&#39;M&#39;) . df[[&#39;A&#39;, &#39;B&#39;]].corr() . A B . A 1.000000 | 0.042426 | . B 0.042426 | 1.000000 | . y_scaled, X_scaled = StandardScaler().fit_transform(df[[&#39;B&#39;]]), StandardScaler().fit_transform(df[[&#39;A&#39;]]) . lin_reg = LinearRegression().fit(X_scaled, y_scaled) . lin_reg.coef_ . array([[0.04242638]]) . Relationship between regression coeficient and coeficient of determination . Simple regression (univariate x) could be written as: . $$ b= frac{ sum left(x_{i}- overline{x} right) left(y_{i}- overline{y} right)}{ sum left(x_{i}- overline{x} right)^{2}} $$ $$ y_{i}=a+b x_{i}+e_{i} $$ The difference from the mean (yi - y) can be decomposed as a sum of two components: . a component corresponding to the difference from the mean of the explanatory variable (xi - x) | an unexplained component described by the residual | . $$ y_{i}- overline{y}=b left(x_{i}- overline{x} right)+e_{i} $$ $$ begin{array}{c}{ sum left(y_{i}- overline{y} right)^{2}=b^{2} sum left(x_{i}- overline{x} right)^{2}+ sum e_{i}^{2}} {S S T=S S E+S S R} end{array} $$ Note: SST = total sum of squares, SSE = explained sum of squares, SSR the sum of squared residuals, but sometimes you can encouter that the meaning is switched and SSE is sum of squared errors and SSR as explained varience . $$ R^{2}= frac{S S E}{S S T}= frac{b^{2} sum left(x_{i}- overline{x} right)^{2}}{ sum left(y_{i}- overline{y} right)^{2}} $$ Coeficient of determination is really just squated of correlation between x and y (holds true just for simple regression) . $$ R^{2}= frac{ left( sum left(x_{i}- overline{x} right) left(y_{i}- overline{y} right) right)^{2}}{ sum left(x_{i}- overline{x} right)^{2} sum left(y_{i}- overline{y} right)^{2}} $$ $$ r_{x y} = frac{ sum_{i=1}^{n} left(x_{i}- overline{x} right) left(y_{i}- overline{y} right)}{ sqrt{ sum_{i=1}^{n} left(x_{i}- overline{x} right)^{2} sum_{i=1}^{n} left(y_{i}- overline{y} right)^{2}}} $$ $$ r_{x y} = sqrt{ R^{2}} $$ $$ R^{2}=1- frac{ sum e_{i}^{2}}{ sum left(y_{i}- overline{y} right)^{2}} $$ $$ R^{2}=1- frac{S S R}{S S T} $$ Check empirically that squared correlation coef. == coef. of determination . df[[&#39;A&#39;, &#39;B&#39;]].corr() ** 2 . A B . A 1.0000 | 0.0018 | . B 0.0018 | 1.0000 | . X, y = df[[&#39;A&#39;]], df[&#39;B&#39;] . lin_reg = LinearRegression().fit(X, y) . r2_score(y,lin_reg.predict(X)) . 0.0017999979073189953 . df_r = pd.DataFrame(data = np.linspace(-1, 1, 10), columns = [&#39;r&#39;]).assign(r2 = lambda x: x[&#39;r&#39;]**2) alt.Chart(df_r).mark_line().encode(x=&#39;r&#39;,y=&#39;r2&#39;).interactive() . Relationship between coeficient of determination and mean squared error . Coeficient of determination is just mean squared error devided by it&#39;s standard deviation . $$ R^{2}(y, hat{y})=1- frac{ sum_{i=0}^{n_{ text { sanples }}-1} left(y_{i}- hat{y}_{i} right)^{2}}{ sum_{i=0}^{n_{ text { samples }}-1} left(y_{i}- overline{y} right)^{2}} $$ $$ operatorname{MSE}(y, hat{y})= frac{1}{n_{ text { samples }}} sum_{i=0}^{n_{ text { samples }}-1} left(y_{i}- hat{y}_{i} right)^{2} $$ Check empirically that mean squared error == coef. of determination scaled by varience . lin_reg = LinearRegression().fit(X, y) . r2_score(y,lin_reg.predict(X)) . 0.0017999979073189953 . 1-(mean_squared_error(y,lin_reg.predict(X)))/np.var(y) . 0.0017999979073189953 .",
            "url": "https://pavelkrizek.github.io/ml-blog/2021/01/10/linear_model_and_stats.html",
            "relUrl": "/2021/01/10/linear_model_and_stats.html",
            "date": " • Jan 10, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a data scientist working at Heidelberg Cement last 3 years on topics such as demand prediction, production planning and prediction of production parameters. Before I have worked at PwC as data analyst. I studied economics and econometrics and I have deep interest in ML explainability/transparency and merging various modeling approaches (Bayesian, classical ML, DL - Keras, Econometrical…) whatever works the best, no bias :D .",
          "url": "https://pavelkrizek.github.io/ml-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pavelkrizek.github.io/ml-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}